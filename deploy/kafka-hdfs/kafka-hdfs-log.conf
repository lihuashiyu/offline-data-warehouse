# 1. 为各组件进行命名
a2.sources=r2
a2.channels=c2
a2.sinks=k2

# 2. 配置 source
a2.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a2.sources.r2.kafka.bootstrap.servers = issac:9092
a2.sources.r2.kafka.topics = mock
a2.sources.r2.batchSize = 10000
a2.sources.r2.batchDurationMillis = 30000

# 时间拦截器，解决零点漂移
a2.sources.r2.interceptors = i1
a2.sources.r2.interceptors.i1.type = interceptor.TimeStampInterceptor$Builder


# 3. 配置 channel
a2.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a2.channels.c2.kafka.bootstrap.servers = issac:9092
a2.channels.c2.kafka.topic = mock
a2.channels.c2.parseAsFlumeEvent = false

a2.channels.c2.type = file
a2.channels.c2.checkpointDir = /offline-data-warehouse/kafka-hdfs/check-point/log/
a2.channels.c2.datadirs = /offline-data-warehouse/kafka-hdfs/data/log/
a2.channels.c2.maxFileSize = 2146435071
a2.channels.c2.capacity = 1000000
a2.channels.c2.keep-alive = 6

# 4. 配置 sink
a2.sinks.k2.type = hdfs
a2.sinks.k2.hdfs.path = /warehouse/log/%Y-%m-%d
# a2.sinks.k2.hdfs.filePrefix = log-
a2.sinks.k2.hdfs.round = false

# 4.1 控制生成的小文件
a2.sinks.k2.hdfs.rollInterval = 360
a2.sinks.k2.hdfs.rollSize = 67108864
a2.sinks.k2.hdfs.rollCount = 0

# 4.2 控制输出文件是原生文件
a2.sinks.k2.hdfs.fileType = CompressedStream
a2.sinks.k2.hdfs.codeC = gzip


# 5. 进行拼接组件，绑定 source 和 channel 以及 sink 和 channel 的关系
a2.sources.r2.channels = c2
a2.sinks.k2.channel= c2
